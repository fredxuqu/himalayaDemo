1. Install
	http://www.apache.org/dyn/closer.cgi/lucene/solr/6.2.1
	download from https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/6.2.1/
	
	unzip 
	https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/solr/6.2.1/solr-6.2.1.tgz
	
	source code
	https://github.com/apache/lucene-solr

2. Config dataSource for DIH
	<dataConfig>
	    <dataSource class="com.alibaba.druid.pool.DruidDataSource" 
					driver="oracle.jdbc.driver.OracleDriver" 
					url="jdbc:oracle:thin:@localhost:1521:orcl" 
					user="scott" 
					password="sa" 
					name="druidDS"/>
					
		<dataSource name="jndiDataSource" jndiName="java:comp/env/jdbc/jndiDS" type="JdbcDataSource"/>
		
	    <document>
	        <entity dataSource="druidDS" name="tagEntity" pk="ID" 
					query="select id,tag_name,type,status,rank,modify_time from tb_social_statistics "
					deltaImportQuery="select id,tag_name,type,status,rank,modify_time from tb_social_statistics where id = ${dih.delta.ID}"
					deltaQuery="select id from tb_social_statistics where modify_time > to_date('${dataimporter.last_index_time}','yyyy-mm-dd HH24:SS:MI')" >
	                
	            <field column="ID" name="id" />
				<field column="TAG_NAME" name="tagName" />
				<field column="TYPE" name="type" />
				<field column="STATUS" name="status" />
				<field column="RANK" name="rank" />
				<field column="MODIFY_TIME" name="modifyTime" />            
	        </entity>
	    </document>
	</dataConfig>	

	NOTE: column name should be uppercase. 
	
3. schema.xml configuration.
	type:
	<fieldType name="string" class="solr.StrField" sortMissingLast="true" />
	<fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0"/>
	<fieldType name="random" class="solr.RandomSortField" indexed="true" />
	<fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
      </analyzer>
    </fieldType>
    <fieldType name="text_general" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <!-- in this example, we will only use synonyms at query time
        <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
        -->
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>
    
    descriptions:
	    name: identity
	    class： 
	    sortMissingLast
	    sortMissingFirst
	    anlyzer
	    type
	    tokenizer
	    filter
	    
	b.	fields: 
		field: fixed fields
		dynamicField:
		copyField 
			multiValued=true
			name
			class
			indexed
			stored: if field is not indexed, it must be stored
			omitNorms
			termVectors
			compressed
			multiValued
			positionIncrementGap
	d.	uniqueKey
	e.	defaultSearchField
	f.	solrQueryParser: and/or
			

4. solrconfig.xml
	solr performance factors
		useCompoundFile
		ramBufferSizeMB
		maxBufferedDocs
		mergeFactor
		maxIndexingThreads: Max number of IndexWriter tasks.
		lockType: single, native, JVM, simple
		unlockOnStartup
	
	query
		maxBooleanClauses
		filterCache: filter cache
		queryResultCache: cache searching result, a list of document ids;
		documentCache: 
		fieldValueCache: field cache, quick search by document id, will be created by default.
		enableLazyFieldLoading
		queryResultWindowSize
		queryResultMaxDocsCached
		listener
		useColdSearcher
		maxWarmingSearchers
		
		
		
	BooleanQuery
	

3. config JNDI
	a. 	copy jetty-jndi-9.3.8.v20160314.jar and jetty-plus-9.3.8.v20160314.jar to solr_home/server/lib
	b.	change druid source code and override setPassword method, then copy druid.jar and oracle driver to solr_home/server/lib/ext
	c.	add below config to solr_home/server/contexts/solr-jetty-context.xml
		<New id="pysearch" class="org.eclipse.jetty.plus.jndi.Resource">
			<Arg></Arg>
			<Arg>jdbc/jndiDS</Arg>
			<Arg>
				<New class="com.alibaba.druid.pool.DruidDataSource">
					<Set name="driverClassName">oracle.jdbc.driver.OracleDriver</Set>  
					<Set name="url">jdbc:oracle:thin:@localhost:1521:orcl</Set>  
					<Set name="username">scott</Set>
					<Set name="password">{MD5}9fdd42c2a7390d3</Set>
				</New>
			</Arg>  
		</New>
	Note: sometimes, you must only use jndiDS to reference the JNDI data source.
		
	d.	add below config to solr-6.3.0\server\solr-webapp\webapp\WEB-INF\web.xml
		<resource-ref>
			<description>Text JNDI Reference</description>
			<res-ref-name>textjndi</res-ref-name>
			<res-type>java.lang.String</res-type>
			<res-auth>Container</res-auth>
		</resource-ref>
		
3.1 jndi for tomcat
	
	<Resource 
         name="jdbc/pysearch"
         auth="Container"
         factory="com.alibaba.druid.pool.DruidDataSourceFactory" 
         type="javax.sql.DataSource"
         driverClass="oracle.jdbc.driver.OracleDriver"
         url="jdbc:oracle:thin:@localhost:1521:orcl"
         username="scott" 
         password="sa"
         maxActive="50"
         maxWait="10000"
         removeabandoned="true"
         removeabandonedtimeout="60"
         logabandoned="false"
         filters="stat"/>
    
   
   	<!--dataSource driver="oracle.jdbc.driver.OracleDriver" 
				url="jdbc:oracle:thin:@localhost:1521:orcl" 
				user="scott" 
				password="sa" 
				name="ds_tag_searching"/--> 
								
	<dataSource jndiName="java:comp/env/jdbc/pysearch" type="JdbcDataSource" name="pysearchDS"/>
	

4. Config smartcn
	
4. Config IK analysis
	config ik analyser for solr (never succ)
		download related ikanalyser config
		copy ik-analyzer-solr-6.3.0.jar to \solr-6.3.0\server\solr-webapp\webapp\WEB-INF\lib
		
		copy IKAnalyzer.cfg.xml、mydict.dic、stopword.dic to 
		/usr/local/solr-6.3.0/server/solr-webapp/webapp/WEB-INF/classes
		
	add below config to managed-schema
	<fieldType name="text_ik" class="solr.TextField" >
		<analyzer type="index" >
			<tokenizer class="org.wltea.analyzer.lucene.IKTokenizerFactory" useSmart="false" conf="ik.conf"/>
			<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
		</analyzer>
		<analyzer type="query">
			<tokenizer class="org.wltea.analyzer.lucene.IKTokenizerFactory" useSmart="false" conf="ik.conf"/>
			<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
		</analyzer>
	</fieldType>
	
	<field name="text_ik"  type="text_ik" indexed="true"  stored="true"  multiValued="true" />
	
5. Config synonyms
	add synonyms filter
	<!-- 集成IK分词器 -->
	<fieldType name="text_ik" class="solr.TextField">
	   <analyzer type="index">
	     <tokenizer class="org.wltea.analyzer.lucene.IKTokenizerFactory" isMaxWordLength="false" useSmart="false" />
		<filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true" />
		<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true" />
		<filter class="solr.LowerCaseFilterFactory" />
	  </analyzer>
	  <analyzer type="query">
	     <tokenizer class="org.wltea.analyzer.lucene.IKTokenizerFactory" isMaxWordLength="True" useSmart="false" />
		<filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true" />
		<filter class="solr.LowerCaseFilterFactory" />
	   </analyzer>
	</fieldType>
	
	update core configuration
	core\conf\synonyms.txt
	add synonyms items
	深圳,深圳市,鹏城
	长沙,长沙市,长沙县,星城
	南京,金陵,建康
	台湾,台灣,臺灣
	
	new BufferedWriter(new OutputStreamWriter(new FileOutputStream(new File("E:/solr/solr-6.3.0/server/solr/iktest/conf/synonyms.txt"),false), "utf-8"));
	to write synonyms.txt file
		
4. solr6.3.0 + tomcat8 
	blog:	http://blog.csdn.net/yzl_8877/article/details/53199355
	a.  download and unzip solr
	b.	copy solr-6.3.0\server\solr-webapp\webapp to tomcat_home\webapps, then change name to solr
	c.	create solr_home under solr-6.3.0\server\solr-webapp\webapp to tomcat_home\webapps\solr
	d.	copy all files under solr-6.3.0\server\solr to solr_home
	e.	copy all jars under solr-6.3.0\server\lib\ext to webapps\solr\WEB-INF\lib
	f.	copy solr-6.3.0\server\resources\log4j.properties to webapps\solr\WEB-INF\classes
	g.	modify webapps\solr\WEB-INF\web.xml and open <evn-entry>, change as below
		<env-entry>
	       <env-entry-name>solr/home</env-entry-name>
	       <env-entry-value>E:\solr\apache-tomcat-8\webapps\solr\solr_home</env-entry-value>
	       <env-entry-type>java.lang.String</env-entry-type>
	    </env-entry>
	h. 	modify tomcat\conf\server.xml
		<Context path="solr" docBase="E:\solr\apache-tomcat-8\webapps\solr" sessionCookieName="ksessionid"/>

	i.	remove <auth-constraint /> from webapps\solr\WEB-INF\web.xml
	j.	http://127.0.0.1:8080/solr/index.html

	set SOLR TIME ZONE
	set SOLR_TIMEZONE=UTC/GMT+08:00
	
	-Dfile.encoding=UTF-8
	

5. How to import csv file
	1. add CSVHandler to solrconfig.xml
	<requestHandler name="/update/csv" class="solr.CSVRequestHandler" startup="lazy">
		<lst name="defaults">
	   		<str name="separator">;</str>
	   		<str name="header">true</str>
	   		<str name="skip">publish_date</str>
	   		<str name="encapsulator">"</str>
		</lst>
  	</requestHandler>
		
	2. add config to schema-manage
		<field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
   		<field name="newscontent" type="text_general" indexed="true" stored="true" />
		
	3. prepare csv file 
		id,newscontent
		1,"Hello World"
		2,"solr 世界美好"
		3,"软件开发,java技术"
	
	4. execute command to import
		http://localhost:8983/solr/news/update?stream.file=E:/solr/solr-6.3.0/example/exampledocs/news.csv&stream.contentType=text/csv;charset=utf-8&fieldnames=id,newscontent&overwrite=false&commit=true

6. solr searching grammar
	1. indexing
		SolrInputDocument doc = new SolrInputDocument();
		doc.setField("id","abc");
		doc.setField("content","hi jack");
		server.add(doc);
		server.commit;
		
		server.deleteById(id);
		server.deleteById(ids);
		server.deleteByQuery("*:*");
		
		SolrQuery query = new SolrQuery();
		query.set("q","*.*");
		QueryResponse rsp =server.query(query)
		SolrDocumentList list = rsp.getResults();
		for (int i = 0; i < list.size(); i++) {
			SolrDocument sd = list.get(i);
			String id = (String) sd.getFieldValue("id");
			System.out.println(id);
		}
		
	2. optimize solr index
		q:	查询字符串，必须的。
		fq:	filter query。使用Filter Query可以充分利用Filter Query Cache，提高检索性能。作用：在q查询符合结果中同时是fq查询符合的，例如：q=mm&fq=date_time:[20081001 TO 20091031]，找关键字mm，并且date_time是20081001到20091031之间的。
		fl:	field list。指定返回结果字段。以空格“ ”或逗号“,”分隔。
		start:	用于分页定义结果起始记录数，默认为0。
		rows 用于分页定义结果每页返回记录数，默认为10。
		sort 排序，格式:sort=<field name>+<desc|asc>[,<field name>+<desc|asc>]… 。示例：（inStock desc, price asc）表示先 “inStock” 降序, 再 “price” 升序，默认是相关性降序。
		df 默认的查询字段，一般默认指定。
		q.op 覆盖schema.xml的defaultOperator（有空格时用"AND"还是用"OR"操作逻辑），一般默认指定。必须大写
		wt writer type。指定查询输出结构格式，默认为“xml”。在solrconfig.xml中定义了查询输出格式：xml、json、python、ruby、php、phps、custom。
		qt query type，指定查询使用的Query Handler，默认为“standard”。
		explainOther 设置当debugQuery=true时，显示其他的查询说明。
		defType	设置查询解析器名称。
				defType=edismax&boost=rank
		boost	即通过boost参数可以在原有的评分基础上再乘以这个参数，该参数可以为某个field,如果rank的值为2则提升2倍
		timeAllowed	设置查询超时时间。
		omitHeader	设置是否忽略查询结果返回头信息，默认为“false”。
		indent	返回的结果是否缩进，默认关闭，用 indent=true|on 开启，一
				般调试json,php,phps,ruby输出才有必要用这个参数。
		version	查询语法的版本，建议不使用它，由服务器指定默认值。
		debugQuery	设置返回结果是否显示Debug信息。
		
	3. 查询时为字段设置权重
		defType=edismax
			qf fieldname1*2 fieldname2*1  
		defType=edismax&qf=fieldname1^2 fieldname2^1
		按照score进行排序的时候，会优先显示fieldname1字段匹配到的记录。
		注意，q里面不能规定查询字段，否则为字段设置的权重就不会起作用。
	
	4. subQuery
	
	5. left match
		fileType of tagNameStr should be string
		{!prefix f=tagNameStr}长沙市开福
		tagNameStr:长沙市开福*
		
	1. keep search order
		localhost:8983/solr/news/select/q=newscontent="南华"~8&fl=id,newscontent&sort=score desc&fq=status:2&start=0&rows=10&defType=complexphrase&wt=json&indent=true
		
		http://localhost:8983/solr/news/select?q=newscontent:"上海"~8&fl=id,newscontent&sort=score desc&start=0&rows=10&defType=complexphrase&wt=json&indent=true
	
7. configure and use spatial search


8. Solr Master and Slave mode
	<requestHandler name="/replication" class="solr.ReplicationHandler" >    
	 	<lst name="master">  
	   	<!-- 执行commit操作后进行replicate操作同样的设置startup, commit, optimize-->  
		   	<str name="replicateAfter">commit</str>
		   	<!-- 执行startup操作后进行replicate操作-->
		   	<str name="replicateAfter">startup</str>
		   	<!-- 复制索引时也同步以下配置文件-->
		   	<str name="confFiles">managed-schema,stopwords.txt</str>
		   	<!-- 验证信息, 由用户自定义用户名-->
		   	<!-- 验证信息, 由用户自定义密码
		   	<str name="httpBasicAuthUser">root</str>
		   	<str name="httpBasicAuthPassword">root123</str>-->
	 	</lst>  
	</requestHandler> 
	
	<requestHandler name="/replication" class="solr.ReplicationHandler" >    
     	<lst name="slave">  
       		<!-- 主服务器的URL, 对于多核同步配置,一一对应即可-->  
       		<str name="masterUrl">http://127.0.0.1:8983/solr/tags</str> 
       		<!-- 60秒进行一次同步操作-->    
       		<str name="pollInterval">00:00:60</str>  
       		<!-- 压缩机制,来传输索引, 可选internal|external, internal内网, external外网-->    
       		<str name="compression">internal</str>  
       		<!-- 设置超时时间-->    
       		<str name="httpConnTimeout">50000</str>     
       		<str name="httpReadTimeout">500000</str>     
       		<!-- 验证信息, 要与master服务器匹配   
       		<str name="httpBasicAuthUser">root</str>     
       		<str name="httpBasicAuthPassword">root123</str> -->     
     	</lst>  
	</requestHandler>
	
	
9. Solr swap
	http://localhost:8983/solr/admin/cores?action=SWAP&core=tagmaster&other=tagslave



SOLR使用手册之查询语法

博客分类： 经验之谈实用文档
 
 
一.基本查询
q – 查询字符串，必须的。
fl – 指定返回那些字段内容，用逗号或空格分隔多个。
start – 返回第一条记录在完整找到结果中的偏移位置，0开始，一般分页用。
rows – 指定返回结果最多有多少条记录，配合start来实现分页。
sort – 排序，格式：sort=<field name>+<desc|asc>[,<field name>+<desc|asc>]… 。示例：（inStock desc, price asc）表示先 “inStock” 降序, 再 “price” 升序，默认是相关性降序。
wt – (writer type)指定输出格式，可以有 xml, json, php, phps, 后面 solr 1.3增加的，要用通知我们，因为默认没有打开。
fq – （filter query）过虑查询，作用：在q查询符合结果中同时是fq查询符合的，例如：q=mm&fq=date_time:[20081001 TO 20091031]，找关键字mm，并且date_time是20081001到20091031之间的。


q.op – 覆盖schema.xml的defaultOperator（有空格时用”AND”还是用”OR”操作逻辑），一般默认指定
df – 默认的查询字段，一般默认指定
qt – （query type）指定那个类型来处理查询请求，一般不用指定，默认是standard。
- 排除在要排除的词前加上 “-” (不包含”号) 号
其它

indent – 返回的结果是否缩进，默认关闭，用 indent=true|on 开启，一般调试json,php,phps,ruby输出才有必要用这个参数。
version – 查询语法的版本，建议不使用它，由服务器指定默认值。
[Solr的检索运算符]
“:” 指定字段查指定值，如返回所有值*:*²
“?”²表示单个任意字符的通配
“*” 表示多个任意字符的通配（不能在检索的项开始使用*或者?符号）²
“~”²表示模糊检索，如检索拼写类似于”roam”的项这样写：roam~将找到形如foam和roams的单词；roam~0.8，检索返回相似度在0.8以上的记录。
²邻近检索，如检索相隔10个单词的”apache”和”jakarta”，”jakarta apache”~10
“^”²控制相关度检索，如检索jakarta apache，同时希望去让”jakarta”的相关度更加好，那么在其后加上”^”符号和增量值，即jakarta^4 apache
布尔操作符AND、||²
布尔操作符OR、²&&
布尔操作符NOT、!、-²（排除操作符不能单独与项使用构成查询）
“+” 存在操作符，要求符号”+”后的项必须在文档相应的域中存在²
( ) 用于构成子查询²
² [] 包含范围检索，如检索某时间段记录，包含头尾，date:[200707 TO 200710]
{}²不包含范围检索，如检索某时间段记录，不包含头尾
date:{200707 TO 200710}
” 转义操作符，特殊字符包括+ – & | ! ( ) { } [ ] ^ ” ~ * ? : “
二.高亮
hl-highlight，h1=true，表示采用高亮。可以用h1.fl=field1,field2 来设定高亮显示的字段。
hl.fl: 用空格或逗号隔开的字段列表。要启用某个字段的highlight功能，就得保证该字段在schema中是stored。如果该参数未被给出，那么就会高 亮默认字段 standard handler会用df参数，dismax字段用qf参数。你可以使用星号去方便的高亮所有字段。如果你使用了通配符，那么要考虑启用 hl.requiredFieldMatch选项。
hl.requireFieldMatch:
如果置为true，除非该字段的查询结果不为空才会被高亮。它的默认值是false，意味 着它可能匹配某个字段却高亮一个不同的字段。如果hl.fl使用了通配符，那么就要启用该参数。尽管如此，如果你的查询是all字段（可能是使用 copy-field 指令），那么还是把它设为false，这样搜索结果能表明哪个字段的查询文本未被找到
hl.usePhraseHighlighter:
如果一个查询中含有短语（引号框起来的）那么会保证一定要完全匹配短语的才会被高亮。
hl.highlightMultiTerm
如果使用通配符和模糊搜索，那么会确保与通配符匹配的term会高亮。默认为false，同时hl.usePhraseHighlighter要为true。
hl.snippets：
这是highlighted片段的最大数。默认值为1，也几乎不会修改。如果某个特定的字段的该值被置为0（如f.allText.hl.snippets=0），这就表明该字段被禁用高亮了。你可能在hl.fl=*时会这么用。
hl.fragsize:
每个snippet返回的最大字符数。默认是100.如果为0，那么该字段不会被fragmented且整个字段的值会被返回。大字段时不会这么做。
hl.mergeContiguous:
如果被置为true，当snippet重叠时会merge起来。
hl.maxAnalyzedChars:
会搜索高亮的最大字符，默认值为51200，如果你想禁用，设为-1
hl.alternateField:
如果没有生成snippet（没有terms 匹配），那么使用另一个字段值作为返回。
hl.maxAlternateFieldLength:
如果hl.alternateField启用，则有时需要制定alternateField的最大字符长度，默认0是即没有限制。所以合理的值是应该为
hl.snippets * hl.fragsize这样返回结果的大小就能保持一致。
hl.formatter:一个提供可替换的formatting算法的扩展点。默认值是simple，这是目前仅有的选项。
显然这不够用，你可以看看org.apache.solr.highlight.HtmlFormatter.java 和 solrconfig.xml中highlighting元素是如何配置的。
注意在不论原文中被高亮了什么值的情况下，如预先已存在的em tags，也不会被转义，所以在有时会导致假的高亮。
hl.fragmenter:
这个是solr制 定fragment算法的扩展点。gap是默认值。regex是另一种选项，这种选项指明highlight的边界由一个正则表达式确定。这是一种非典型 的高级选项。为了知道默认设置和fragmenters (and formatters)是如何配置的，可以看看solrconfig.xml中的highlight段。
regex 的fragmenter有如下选项：
hl.regex.pattern:正则表达式的pattern
hl.regex.slop:这是hl.fragsize能变化以适应正则表达式的因子。默认值是0.6，意思是如果hl.fragsize=100那么fragment的大小会从40-160.
三.分组查询：
 
1.Field Facet
Facet 字段通过在请求中加入 ”facet.field” 参数加以声明 , 如果需要对多个字段进行 Facet查询 , 那么将该参数声明多次 . 比如
/select?q=联想
&facet=on
&facet.field=cpu
&facet.field=videoCard

各个 Facet 字段互不影响 , 且可以针对每个 Facet 字段设置查询参数 . 以下介绍的参数既可以应用于所有的 Facet 字段 , 也可以应用于每个单独的 Facet 字段 . 应用于单独的字段时通过
f.字段名.参数名=参数值
这种方式调用 . 比如 facet.prefix 参数应用于 cpu 字段 , 可以采用如下形式
f.cpu.facet.prefix=Intel
1.1   facet.prefix
表示 Facet 字段值的前缀 . 比如 ”facet.field=cpu&facet.prefix=Intel”, 那么对 cpu字段进行 Facet 查询 , 返回的 cpu 都是以 ”Intel” 开头的 ,”AMD” 开头的 cpu 型号将不会被统计在内 .
1.2   facet.sort
表示 Facet 字段值以哪种顺序返回 . 可接受的值为 true(count)|false(index,lex). true(count) 表示按照 count 值从大到小排列 . false(index,lex) 表示按照字段值的自然顺序 (字母 , 数字的顺序 ) 排列 . 默认情况下为 true(count). 当 facet.limit 值为负数时 ,默认 facet.sort= false(index,lex).
1.3   facet.limit
限制 Facet 字段返回的结果条数 . 默认值为 100. 如果此值为负数 , 表示不限制 .
1.4   facet.offset
返回结果集的偏移量 , 默认为 0. 它与 facet.limit 配合使用可以达到分页的效果 .
1.5   facet.mincount
限制了 Facet 字段值的最小 count, 默认为 0. 合理设置该参数可以将用户的关注点集中在少数比较热门的领域 .
1.6   facet.missing
默认为 ””, 如果设置为 true 或者 on, 那么将统计那些该 Facet 字段值为 null 的记录.
1.7   facet.method
取值为 enum 或 fc, 默认为 fc. 该字段表示了两种 Facet 的算法 , 与执行效率相关 .
enum 适用于字段值比较少的情况 , 比如字段类型为布尔型 , 或者字段表示中国的所有省份.Solr 会遍历该字段的所有取值 , 并从 filterCache 里为每个值分配一个 filter( 这里要求 solrconfig.xml 里对 filterCache 的设置足够大 ). 然后计算每个 filter 与主查询的交集 .
fc( 表示 Field Cache) 适用于字段取值比较多 , 但在每个文档里出现次数比较少的情况 .Solr 会遍历所有的文档 , 在每个文档内搜索 Cache 内的值 , 如果找到就将 Cache 内该值的count 加 1.
1.8   facet.enum.cache.minDf
当 facet.method=enum 时 , 此参数其作用 ,minDf 表示 minimum document frequency. 也就是文档内出现某个关键字的最少次数 . 该参数默认值为 0. 设置该参数可以减少 filterCache 的内存消耗 , 但会增加总的查询时间 ( 计算交集的时间增加了 ). 如果设置该值的话 ,官方文档建议优先尝试 25-50 内的值 .
2.       Date Facet
日期类型的字段在文档中很常见 , 如商品上市时间 , 货物出仓时间 , 书籍上架时间等等 . 某些情况下需要针对这些字段进行 Facet. 不过时间字段的取值有无限性 , 用户往往关心的不是某个时间点而是某个时间段内的查询统计结果 . Solr 为日期字段提供了更为方便的查询统计方式 .当然 , 字段的类型必须是 DateField( 或其子类型 ).
需要注意的是 , 使用 Date Facet 时 , 字段名 , 起始时间 , 结束时间 , 时间间隔这 4 个参数都必须提供 .
与 Field Facet 类似 ,Date Facet 也可以对多个字段进行 Facet. 并且针对每个字段都可以单独设置参数 .
2.1   facet.date
该参数表示需要进行 Date Facet 的字段名 , 与 facet.field 一样 , 该参数可以被设置多次 , 表示对多个字段进行 Date Facet.
2.2   facet.date.start
起始时间 , 时间的一般格式为 ” 1995-12-31T23:59:59Z”, 另外可以使用 ”NOW”,”YEAR”,”MONTH” 等等 , 具体格式可以参考 org.apache.solr.schema. DateField 的 java doc.
2.3   facet.date.end
结束时间 .
2.4   facet.date.gap
时间间隔 . 如果 start 为 2009-1-1,end 为 2010-1-1.gap 设置为 ”+1MONTH” 表示间隔1 个月 , 那么将会把这段时间划分为 12 个间隔段 . 注意 ”+” 因为是特殊字符所以应该用 ”%2B” 代替 .
2.5   facet.date.hardend
取值可以为 true|false, 默认为 false. 它表示 gap 迭代到 end 处采用何种处理 . 举例说明 start 为 2009-1-1,end 为 2009-12-25,gap 为 ”+1MONTH”,hardend 为 false 的话最后一个时间段为 2009-12-1 至 2010-1-1;hardend 为 true 的话最后一个时间段为 2009-12-1 至 2009-12-25.
2.6   facet.date.other
取值范围为 before|after|between|none|all, 默认为 none.
before 会对 start 之前的值做统计 .
after 会对 end 之后的值做统计 .
between 会对 start 至 end 之间所有值做统计 . 如果 hardend 为 true 的话 , 那么该值就是各个时间段统计值的和 .
none 表示该项禁用 .
all 表示 before,after,all 都会统计 .
举例 :
&facet=on
&facet.date=date
&facet.date.start=2009-1-1T0:0:0Z
&facet.date.end=2010-1-1T0:0:0Z
&facet.date.gap=%2B1MONTH
&facet.date.other=all
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries"/>
         <lst name="facet_fields"/>
         <lst name="facet_dates">
<int name="2009-01-01T00:00:00Z">5</int>
<int name="2009-11-01T00:00:00Z">1</int>
<int name="2009-12-01T00:00:00Z">5</int>
<str name="gap">+1MONTH</str>
<date name="end">2010-01-01T00:00:00Z</date>
<int name="before">180</int>
<int name="after">5</int>
<int name="between">54</int>
</lst>
</lst>
3.       Facet Query
Facet Query 利用类似于 filter query 的语法提供了更为灵活的 Facet. 通过 facet.query 参数 , 可以对任意字段进行筛选 .
例 1:
&facet=on
&facet.query=date:[2009-1-1T0:0:0Z TO 2009-2-1T0:0:0Z]
&facet.query=date:[2009-4-1T0:0:0Z TO 2009-5-1T0:0:0Z]
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries">
                   <int name="date:[2009-1-1T0:0:0Z TO 2009-2-1T0:0:0Z]">5</int>
<int name="date:[2009-4-1T0:0:0Z TO 2009-5-1T0:0:0Z]">3</int>
</lst>
         <lst name="facet_fields"/>
         <lst name="facet_dates"/>
</lst>
例 2:
&facet=on
&facet.query=date:[2009-1-1T0:0:0Z TO 2009-2-1T0:0:0Z]
&facet.query=price:[* TO 5000]
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries">
                   <int name="date:[2009-1-1T0:0:0Z TO 2009-2-1T0:0:0Z]">5</int>
<int name="price:[* TO 5000]">116</int>
</lst>
         <lst name="facet_fields"/>
         <lst name="facet_dates"/>
</lst>
例 3:
&facet=on
&facet.query=cpu:[A TO G]
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries">
                   <int name="cpu:[A TO G]">11</int>
</lst>
         <lst name="facet_fields"/>
         <lst name="facet_dates"/>
</lst>
4.       key 操作符
可以用 key 操作符为 Facet 字段取一个别名 .
例 :
&facet=on
&facet.field={!key=中央处理器}cpu
&facet.field={!key=显卡}videoCard
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries"/>
         <lst name="facet_fields">
                   <lst name="中央处理器">
                            <int name="Intel 酷睿2双核 T6600">48</int>
                            <int name="Intel 奔腾双核 T4300">28</int>
 
                   </lst>
                   <lst name="显卡">
                            <int name="ATI Mobility Radeon HD 4">63</int>
                            <int name="NVIDIA GeForce G 105M">24</int>
<int name="NVIDIA GeForce GT 240M">21</int>
<int name="NVIDIA GeForce G 103M">8</int>
<int name="NVIDIA GeForce GT 220M">8</int>
<int name="NVIDIA GeForce 9400M G">7</int>
<int name="NVIDIA GeForce G 210M">6</int>
</lst>
         </lst>
         <lst name="facet_dates"/>
</lst>
5.       tag 操作符和 ex 操作符
当查询使用 filter query 的时候 , 如果 filter query 的字段正好是 Facet 字段 , 那么查询结果往往被限制在某一个值内 .
例 :
&fq=screenSize:14
&facet=on
&facet.field=screenSize
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries"/>
         <lst name="facet_fields">
                   <lst name=" screenSize">
                            <int name="14.0">107</int>
<int name="10.2">0</int>
<int name="11.1">0</int>
</lst>
         </lst>
         <lst name="facet_dates"/>
</lst>
可以看到 , 屏幕尺寸 (screenSize) 为 14 寸的产品共有 107 件 , 其它尺寸的产品的数目都是0, 这是因为在 filter 里已经限制了 screenSize:14. 这样 , 查询结果中 , 除了 screenSize=14 的这一项之外 , 其它项目没有实际的意义 .
有些时候 , 用户希望把结果限制在某一范围内 , 又希望查看该范围外的概况 . 比如上述情况 ,既要把查询结果限制在 14 寸屏的笔记本 , 又想查看一下其它屏幕尺寸的笔记本有多少产品 . 这个时候需要用到 tag 和 ex 操作符 .
tag 就是把一个 filter 标记起来 ,ex(exclude) 是在 Facet 的时候把标记过的 filter 排除在外 .
例 :
&fq={!tag=aa}screenSize:14
&facet=on
&facet.field={!ex=aa}screenSize
返回结果 :
<lst name="facet_counts">
         <lst name="facet_queries"/>
         <lst name="facet_fields">
                   <lst name=" screenSize">
                            <int name="14.0">107</int>
<int name="14.1">40</int>
<int name="13.3">34</int>
</lst>
         </lst>
         <lst name="facet_dates"/>
</lst>

	
	
